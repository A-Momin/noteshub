<details open><summary style="font-size:22px;color:Orange">Data Analysis Terms and Concepts</summary>

Data analysis is a vast field encompassing various stages, techniques, and methodologies to transform raw data into meaningful insights. Here is a detailed explanation of the key terms and concepts in data analysis:

1. <b style="font-size:18px;color:#C71585">Data Collection</b>: The process of gathering data from various sources to be used for analysis.

    - **Sources**: Databases, surveys, sensors, web scraping, APIs, social media, and more.
    - **Techniques**: Manual data entry, automated data collection tools, and data import functions.

2. <b style="font-size:18px;color:#C71585">Data Cleaning (Data Cleansing)</b>: The process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset.

    - **Handling Missing Values**: Methods include deletion, imputation (mean, median, mode), and using algorithms to predict missing values.
    - **Correcting Errors**: Identifying and fixing typographical errors, inconsistencies, and inaccuracies.
    - **Standardizing Formats**: Ensuring consistent data formats, such as date and time formats.
    - **Removing Duplicates**: Identifying and removing duplicate records to ensure data integrity.

3. <b style="font-size:18px;color:#C71585">Data Transformation</b>: The process of converting data from one format or structure to another to make it suitable for analysis.

    - **Normalization**: Scaling data to a standard range (e.g., 0 to 1) to ensure uniformity.
    - **Aggregation**: Summarizing data, such as calculating sums, averages, or counts.
    - **Encoding**: Converting categorical data into numerical format (e.g., one-hot encoding).
    - **Feature Engineering**: Creating new variables (features) that may improve model performance.

4. <b style="font-size:18px;color:#C71585">Data Integration</b>: The process of combining data from different sources to provide a unified view.

    - **Merging Datasets**: Combining data based on common fields.
    - **Resolving Conflicts**: Addressing inconsistencies and discrepancies between datasets.
    - **Ensuring Consistency**: Maintaining data integrity across combined data sources.

5. <b style="font-size:18px;color:#C71585">Data Reduction</b>: The process of reducing the amount of data to be processed while retaining its significant information.

    - **Sampling**: Selecting a representative subset of the data.
    - **Feature Selection**: Choosing the most relevant variables (features) for analysis.
    - **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) to reduce the number of variables.

6. <b style="font-size:18px;color:#C71585">Data Wrangling</b>: Data Wrangling (also known as Data Munging) is the process of transforming and mapping raw data into a more usable format. This process typically involves:

    - **Data Collection**: Gathering data from various sources, such as databases, APIs, files, or web scraping.
    - **Data Cleaning**: Identifying and correcting errors or inconsistencies in the data, such as handling missing values, correcting typos, and removing duplicates.
    - **Data Transformation**: Converting data into the desired format, which may include normalization, scaling, and encoding categorical variables.
    - **Data Integration**: Combining data from multiple sources into a coherent dataset.
    - **Data Reduction**: Reducing the volume of data, which may include techniques like sampling, filtering, or aggregation to focus on the most relevant information.

7. <b style="font-size:18px;color:#C71585">Data Exploration (Exploratory Data Analysis - EDA)</b>: The initial phase of data analysis where the main characteristics of the dataset are understood.

    - **Descriptive Statistics**: Calculating measures such as mean, median, mode, variance, and standard deviation.
    - **Visualization**: Creating histograms, scatter plots, box plots, and other charts to visualize data distribution and relationships.
    - **Summary Statistics**: Providing an overview of the data, including central tendency and dispersion.

8. <b style="font-size:18px;color:#C71585">Data Visualization</b>: The graphical representation of data to help understand trends, patterns, and outliers.

    - **Charts and Graphs**: Bar charts, line charts, pie charts, scatter plots, and more.
    - **Dashboards**: Interactive visual displays that present data in an easy-to-read format.
    - **Geospatial Visualizations**: Maps to represent data with a geographical component.

9. <b style="font-size:18px;color:#C71585">Data Analysis</b>: The process of examining, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making.

    - **Statistical Analysis**: Applying statistical methods to test hypotheses, identify relationships, and make inferences.
    - **Hypothesis Testing**: Determining the significance of observed patterns (e.g., t-tests, chi-square tests).
    - **Regression Analysis**: Examining relationships between variables (e.g., linear regression, logistic regression).
    - **Clustering**: Grouping similar data points together (e.g., K-means, hierarchical clustering).
    - **Classification**: Assigning data points to predefined categories (e.g., decision trees, support vector machines).
    - **Time Series Analysis**: Analyzing data points collected or recorded at specific time intervals.

10. <b style="font-size:18px;color:#C71585">Machine Learning</b>: A subset of data analysis that involves training algorithms to learn patterns in data and make predictions or decisions.

    - **Supervised Learning**: Algorithms trained on labeled data (e.g., linear regression, decision trees).
    - **Unsupervised Learning**: Algorithms trained on unlabeled data (e.g., clustering, association rules).
    - **Reinforcement Learning**: Algorithms that learn through rewards and penalties (e.g., Q-learning).

11. <b style="font-size:18px;color:#C71585">Big Data Analysis</b>: The process of analyzing large, complex datasets that traditional data processing software cannot handle.

    - **Hadoop**: An open-source framework for distributed storage and processing of big data.
    - **Spark**: An open-source unified analytics engine for large-scale data processing.
    - **NoSQL Databases**: Databases designed to handle unstructured and semi-structured data (e.g., MongoDB, Cassandra).

12. <b style="font-size:18px;color:#C71585">Data Warehousing</b>: The process of collecting and managing data from various sources to provide meaningful business insights.

    - **ETL (Extract, Transform, Load)**: The process of extracting data from source systems, transforming it into a suitable format, and loading it into a data warehouse.
    - **Data Marts**: Subsets of data warehouses tailored to specific business lines or departments.
    - **OLAP (Online Analytical Processing)**: Tools that provide fast analysis of data stored in a data warehouse.

13. <b style="font-size:18px;color:#C71585">Data Governance</b>: The management of data availability, usability, integrity, and security in an organization.

    - **Data Policies**: Establishing rules and guidelines for data management.
    - **Data Quality Management**: Ensuring data is accurate, complete, and reliable.
    - **Compliance**: Adhering to legal and regulatory requirements for data protection and privacy.

14. <b style="font-size:18px;color:#C71585">Data Ethics</b>: The consideration of moral issues related to data collection, analysis, and usage.

    - **Privacy**: Ensuring individuals' data is protected and used appropriately.
    - **Bias and Fairness**: Avoiding and mitigating biases in data and algorithms.
    - **Transparency**: Being open about data practices and methodologies.

15. <b style="font-size:18px;color:#C71585">Data Reporting</b>: The process of organizing and summarizing data to communicate findings effectively.

    - **Reporting Software**: Tools like Tableau, Power BI, and Looker for creating reports and dashboards.
    - **Automated Reports**: Scheduled reports that provide regular updates on key metrics.

16. <b style="font-size:18px;color:#C71585">Data Engineering</b>: The development and maintenance of systems and infrastructure for collecting, storing, and processing data.

    - **Pipeline Development**: Creating workflows to move data from source systems to storage and analysis platforms.
    - **Database Management**: Designing and maintaining databases to ensure efficient data storage and retrieval.
    - **Cloud Computing**: Using cloud services like AWS, Azure, and Google Cloud for scalable data storage and processing.

</details>

---

<details open><summary style="font-size:22px;color:Orange">Machine Learning Terms and Concepts</summary>

1. **Supervised Learning**: Supervised learning is a type of machine learning where the model learns from labeled data, meaning each input data point is associated with an output label. The goal is to learn a mapping from inputs to outputs based on the provided examples. Common supervised learning tasks include classification (predicting a category) and regression (predicting a continuous value).

2. **Unsupervised Learning**: Unsupervised learning is a type of machine learning where the model learns from unlabeled data, meaning there are no output labels provided. The goal is to discover patterns, structures, or relationships in the data without explicit guidance. Common unsupervised learning tasks include clustering (grouping similar data points) and dimensionality reduction (reducing the number of input features).

3. **Reinforcement Learning**: Reinforcement learning is a type of machine learning where an agent learns to interact with an environment by taking actions to maximize cumulative rewards. The agent learns through trial and error, receiving feedback from the environment in the form of rewards or penalties. The goal is to learn a policy that maps states to actions in order to achieve long-term objectives.

4. **Model**: A model is a mathematical representation of a system or process that captures the relationships between inputs and outputs. In machine learning, a model learns from data to make predictions or decisions. Examples of models include linear regression, decision trees, neural networks, etc.

5. **Feature**: A feature is an individual measurable property or characteristic of a data point. Features are the input variables used to train a machine learning model. They can be numeric, categorical, or even text-based. Feature engineering involves selecting, transforming, and creating relevant features to improve model performance.

6. **Label**: A label is the output or target variable in supervised learning. It represents the ground truth or correct answer associated with each input data point. In classification tasks, labels are categorical (e.g., class labels), while in regression tasks, labels are continuous (e.g., numerical values).

7. **Training Data**: Training data is the labeled data used to train a machine learning model. It consists of input features and corresponding labels. The model learns from the training data to generalize patterns and relationships that enable it to make predictions on new, unseen data.

8. **Testing Data**: Testing data is separate from the training data and is used to evaluate the performance of a trained machine learning model. It consists of input features and corresponding labels, but the model has not seen the testing data during training. Testing data helps assess the model's ability to generalize to new, unseen examples.

9. **Overfitting**: Overfitting occurs when a machine learning model learns to capture noise or random fluctuations in the training data rather than underlying patterns. As a result, the model performs well on the training data but poorly on unseen data. Overfitting can be mitigated by using techniques such as cross-validation, regularization, and reducing model complexity.

10. **Underfitting**: Underfitting occurs when a machine learning model is too simple to capture the underlying structure of the data. The model fails to learn from the training data and performs poorly on both the training and testing data. Underfitting can be addressed by using more complex models, adding more features, or increasing the model's capacity.

11. **Bias-Variance Tradeoff**: The bias-variance tradeoff is a fundamental concept in machine learning that refers to the balance between bias (error due to underfitting) and variance (error due to overfitting). A model with high bias tends to oversimplify the data, while a model with high variance captures noise in the data. Finding the right balance between bias and variance is essential for building models that generalize well to new data.

12. **Cross-Validation**: Cross-validation is a technique used to assess the performance of a machine learning model. It involves partitioning the training data into multiple subsets, training the model on different combinations of subsets, and evaluating its performance on the remaining data. Cross-validation helps estimate the model's generalization error and identify potential issues such as overfitting.

13. **Hyperparameters**: Hyperparameters are configuration settings that are not learned from the data but are set before training a machine learning model. Examples of hyperparameters include learning rate, regularization strength, and the number of hidden layers in a neural network. Tuning hyperparameters is an essential step in optimizing model performance.

14. **Gradient Descent**: Gradient descent is an optimization algorithm used to minimize the loss function and update the parameters of a machine learning model. It works by iteratively adjusting the model parameters in the direction of the steepest descent of the loss function gradient. Gradient descent is widely used in training neural networks and other iterative optimization problems.

15. **Loss Function**: A loss function measures the difference between the predicted values of a machine learning model and the actual values (labels) in the training data. The goal is to minimize the loss function during training, which indicates how well the model is performing. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy loss for classification tasks.

16. **Regularization**: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. It discourages the model from learning overly complex patterns

</details>

---

<details open><summary style="font-size:22px;color:Orange">Model Evaluation Matrics</summary>

In machine learning classification, several evaluation metrics are used to assess the performance of a model. These metrics provide insights into how well the model is making predictions. Here are some of the most commonly used evaluation metrics for classification tasks:

#### Regression

#### Classification

1.  **Confusion Matrix**

    A confusion matrix is a table used to describe the performance of a classification model. It shows the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions made by the model.

    |                 | Predicted Positive | Predicted Negative |
    | :-------------- | :----------------- | :----------------- |
    | Actual Positive | TP                 | FN                 |
    | Actual Negative | FP                 | TN                 |

    -   **True Positive (TP)**: The model correctly predicts the positive class.
    -   **True Negative (TN)**: The model correctly predicts the negative class.
    -   **False Positive (FP)**: The model incorrectly predicts the positive class (Type I error).
    -   **False Negative (FN)**: The model incorrectly predicts the negative class (Type II error).

1.  **Accuracy**

    Accuracy is the ratio of correctly predicted instances (both TP and TN) to the total instances.

    Accuracy is useful when the class distribution is balanced but can be misleading for imbalanced datasets.

1.  **Precision**: Precision, also known as positive predictive value, measures the proportion of true positive predictions among all positive predictions.

1.  **Recall (Sensitivity or True Positive Rate)**: Recall measures the proportion of true positive predictions among all actual positive instances.

1.  **F1 Score**: The F1 score is the harmonic mean of precision and recall, providing a balance between the two.

1.  **Specificity (True Negative Rate)**: Specificity measures the proportion of true negative predictions among all actual negative instances.

1.  **ROC Curve and AUC (Area Under the Curve)**: The Receiver Operating Characteristic (ROC) curve plots the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings. The Area Under the Curve (AUC) summarizes the performance of the classifier across all thresholds.

    -   **ROC Curve**: Visual representation of the trade-off between true positive rate and false positive rate.
    -   **AUC**: A single value representing the classifier's ability to distinguish between classes. Higher AUC indicates better performance.

1.  **Log Loss (Cross-Entropy Loss)**: Log loss measures the performance of a classification model whose output is a probability value between 0 and 1. It penalizes false classifications more when they are confident but wrong.

1.  **Hamming Loss**: Hamming loss calculates the fraction of labels that are incorrectly predicted. It is used in multi-label classification.

</details>

---

<details open><summary style="font-size:22px;color:Orange">Loss Functions</summary>

</details>

---

<details open><summary style="font-size:22px;color:Orange">Linear Regression</summary>

Linear regression is a statistical method used to model and analyze the relationships between a dependent variable (often called the target or outcome variable) and one or more independent variables (also known as predictors, features, or explanatory variables). The primary objective of linear regression is to predict the value of the dependent variable based on the values of the independent variables.

### Types of Linear Regression

1. **Simple Linear Regression**: Involves a single independent variable. The relationship between the dependent variable \( Y \) and the independent variable \( X \) is modeled as a straight line:
   \[
   Y = \beta_0 + \beta_1 X + \epsilon
   \]
   Here, \( \beta_0 \) is the intercept, \( \beta_1 \) is the slope of the line, and \( \epsilon \) is the error term.

2. **Multiple Linear Regression**: Involves two or more independent variables. The relationship is modeled as:
   \[
   Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
   \]
   Here, \( \beta_0 \) is the intercept, \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients for the independent variables \( X_1, X_2, \ldots, X_n \), and \( \epsilon \) is the error term.

### Assumptions of Linear Regression

For the results of linear regression to be reliable, several assumptions must be met:

1. **Linearity**: The relationship between the dependent and independent variables is linear.
2. **Independence**: The observations are independent of each other.
3. **Homoscedasticity**: The residuals (errors) have constant variance at every level of the independent variable(s).
4. **Normality**: The residuals of the model are normally distributed.
5. **No Multicollinearity**: In multiple linear regression, the independent variables are not highly correlated with each other.

### Model Fitting

The goal of linear regression is to find the best-fitting line through the data points. This is typically done using the **least squares method**, which minimizes the sum of the squared differences between the observed values and the predicted values (residuals).

The coefficients \( \beta*0, \beta_1, \ldots, \beta_n \) are estimated such that they minimize the cost function:
\[
J(\beta_0, \beta_1, \ldots, \beta_n) = \sum*{i=1}^{m} (Y*i - (\beta_0 + \beta_1 X*{i1} + \beta*2 X*{i2} + \ldots + \beta*n X*{in}))^2
\]
where \( m \) is the number of observations.

### Evaluation Metrics

Several metrics are used to evaluate the performance of a linear regression model:

1. **R-squared (\( R^2 \))**: Indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.
2. **Adjusted R-squared**: Adjusts \( R^2 \) for the number of predictors in the model, providing a more accurate measure when multiple predictors are used.
3. **Mean Squared Error (MSE)**: The average of the squared differences between the observed and predicted values.
4. **Root Mean Squared Error (RMSE)**: The square root of MSE, providing a measure of the average error magnitude.
5. **Mean Absolute Error (MAE)**: The average of the absolute differences between the observed and predicted values.

### Summary

Linear regression is a fundamental and widely used method for predicting the relationship between variables. By fitting a linear equation to the observed data, it provides insights into how changes in the independent variables affect the dependent variable. Understanding and correctly applying the assumptions and evaluation metrics is crucial for building effective linear regression models.

</details>

---

<details open><summary style="font-size:22px;color:Orange">Logistic Regression</summary>

</details>

---

<details open><summary style="font-size:22px;color:Orange">Support Vector Machine (SVM)</summary>

Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is particularly effective in high-dimensional spaces and is widely used for tasks such as image classification, text classification, and regression analysis. Here's a detailed explanation of Support Vector Machines:

1. **Introduction**:

    - SVM was developed by Vladimir Vapnik and his colleagues in the 1990s.
    - It falls under the category of discriminative classifiers.

2. **Objective**:

    - The primary objective of SVM is to find a hyperplane that best separates data into different classes in feature space.

3. **Key Concepts**:

    - `Hyperplane`:
        - In a two-dimensional space, a hyperplane is a line that separates the data into different classes.
        - In higher-dimensional spaces, it becomes a plane.
    - `Support Vectors`:
        - Support vectors are the data points that are closest to the decision boundary (hyperplane).
        - They play a crucial role in determining the optimal hyperplane.
    - `Margin`:
        - The margin is the distance between the decision boundary and the nearest data point (support vector).
        - SVM aims to maximize this margin.
    - `Kernel Trick`:
        - SVM can efficiently handle non-linearly separable data by using the kernel trick.
        - Common kernels include linear, polynomial, and radial basis function (RBF) kernels.

4. **Classification**:

    - `Linear SVM`:

        - For linearly separable data, SVM finds the optimal hyperplane that maximizes the margin.
        - The decision function is based on the sign of the dot product between the input vector and the weight vector.

    - `Soft Margin SVM`:

        - In cases where data is not perfectly separable, SVM introduces a penalty term (C) to allow for some misclassification.

    - `Multi-Class Classification`:
        - SVM can be extended for multi-class classification using methods like one-vs-one or one-vs-all.

5. **Regression**:

    SVM can be used for regression tasks by predicting a continuous output instead of discrete classes.

6. **Optimization**:

    - The optimization problem involves finding the weights and bias that define the hyperplane.
    - The objective is to maximize the margin while minimizing the classification error.

7. **Regularization (C parameter)**:

    - The regularization parameter (C) controls the trade-off between maximizing the margin and minimizing the classification error.
    - Larger values of C result in a smaller margin but fewer misclassifications.

8. **Kernel Functions: Commonly used kernels include**:

    - Linear Kernel
    - Polynomial Kernel
    - RBF (Radial Basis Function) Kernel

9. **Advantages**:

    - Effective in high-dimensional spaces.
    - Versatile and can handle non-linear data.

10. **Challenges**:

    - Choice of kernel and hyperparameter tuning can be critical.
    - Sensitivity to outliers.

11. **Applications**:

    - Text classification, image classification, handwriting recognition, face detection, etc.

12. **Summary**:

    - SVM aims to find a hyperplane that best separates data into different classes with a maximum margin.
    - It can handle non-linearly separable data using the kernel trick.
    - The regularization parameter (C) controls the trade-off between margin maximization and classification error minimization.

</details>

---

<details open><summary style="font-size:22px;color:Orange">Decission Tree</summary>

</details>

---

<details open><summary style="font-size:22px;color:Orange">Random Forest</summary>

</details>

---

<details open><summary style="font-size:22px;color:Orange">Support Vector Machine (SVM)</summary>

</details>

---

### Generative AI

Generative AI, short for Generative Artificial Intelligence, refers to a category of artificial intelligence systems and techniques that are designed to generate new, original content or data that is similar to, or resembles, human-created content. These systems have the ability to produce text, images, audio, or even other types of data, often based on patterns and structures they have learned from large datasets during training.

Here are some key aspects and concepts related to generative AI:

-   `Generative Models`: Generative AI systems are often built using generative models, which are mathematical and computational models that learn the underlying patterns and structures of data. These models can then generate new instances of data that share similarities with the training data.

-   `Training Data`: Generative AI models require large datasets for training. For example, a text-based generative model might be trained on a vast collection of text documents, while an image-based model would need a large dataset of images. The quality and diversity of the training data can significantly impact the generated results.

-   `Variety of Applications`: Generative AI has a wide range of applications across various domains. Some common applications include:

    -   `Text Generation`: This includes tasks such as language translation, chatbots, and the generation of creative writing or poetry.

    -   `Image Generation`: Generative models can create realistic images, whether it's generating faces of people who don't exist, enhancing or modifying existing images, or creating entirely new artworks.

    -   `Audio Generation`: In the realm of audio, generative AI can create music, mimic human speech, or even generate sound effects for games and movies.

-   `Types of Generative Models`: There are several types of generative models, with some of the most notable being:

    -   `Generative Adversarial Networks (GANs)`: GANs consist of two neural networks, a generator and a discriminator, that are trained in opposition. The generator aims to create data that is indistinguishable from real data, while the discriminator tries to tell the difference. This adversarial training process leads to the generation of high-quality content.

    -   `Variational Autoencoders (VAEs)`: VAEs are probabilistic models that map data into a lower-dimensional space and then back to the original space. They are used for tasks like image reconstruction and generating new data samples.

    -   `Recurrent Neural Networks (RNNs) and Transformers`: These models are often used for text generation tasks. Transformers, in particular, have become popular due to their effectiveness in various natural language processing applications.

-   `Ethical Considerations`: Generative AI also raises ethical concerns, especially when it comes to generating deepfake videos or misinformation. It's important to use this technology responsibly and consider the potential consequences.

In summary, generative AI is a fascinating field that revolves around the creation of AI systems capable of producing new and creative content across different domains. These systems are increasingly finding applications in industries such as entertainment, healthcare, and marketing, among others, and they continue to advance with ongoing research and development efforts.

---

**Equation 4-1: Linear Regression model prediction**

$$
\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
$$

**Equation 4-2: Linear Regression model prediction (vectorized form)**

$$
\hat{y} = h_{\boldsymbol{\theta}}(\mathbf{x}) = \boldsymbol{\theta} \cdot \mathbf{x}
$$

**Equation 4-3: MSE cost function for a Linear Regression model**

$$
\begin{split}
\text{MSE}(\mathbf{X}, h*{\boldsymbol{\theta}})
&= \dfrac{1}{m} \sum\limits*{i=1}^{m}{(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})^2} \\
&= \dfrac{1}{m} \left\| \mathbf{X} \boldsymbol{\theta} - \mathbf{y}) \right\|^2
\end{split}
$$

**Equation 4-4: Normal Equation**

$$
\hat{\boldsymbol{\theta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$

**Equation 4-5: Partial derivatives of the cost function**

$$
\dfrac{\partial}{\partial \theta_j} \text{MSE}(\boldsymbol{\theta}) = \dfrac{2}{m}\sum\limits_{i=1}^{m}(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})\, x_j^{(i)}
$$

**Equation 4-6: Gradient vector of the cost function**

$$
\nabla_{\boldsymbol{\theta}}\, \text{MSE}(\boldsymbol{\theta}) =
\begin{pmatrix}
 \frac{\partial}{\partial \theta_0} \text{MSE}(\boldsymbol{\theta}) \\
 \frac{\partial}{\partial \theta_1} \text{MSE}(\boldsymbol{\theta}) \\
 \vdots \\
 \frac{\partial}{\partial \theta_n} \text{MSE}(\boldsymbol{\theta})
\end{pmatrix}
 = \dfrac{2}{m} \mathbf{X}^T (\mathbf{X} \boldsymbol{\theta} - \mathbf{y})
$$

**Equation 4-7: Gradient Descent step**

$$
\boldsymbol{\theta}_{new} = \boldsymbol{\theta}_{old} - \eta \cdot \nabla_{\boldsymbol{\theta}}\, \text{MSE}(\boldsymbol{\theta})
$$

**Equation 4-8: Ridge Regression cost function**

$$
\begin{split}
J(\boldsymbol{\theta})
&= \dfrac{1}{m} \sum\limits*{i=1}^{m}{(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})^2} + \alpha \dfrac{1}{2}\sum\limits*{i=1}^{n}{\theta*i}^2 \\
&= \dfrac{1}{m} \left\| \mathbf{X} \boldsymbol{\theta} - \mathbf{y}) \right\|^2 + \dfrac{\alpha}{2}\sum\limits*{i=1}^{n} \left\| \boldsymbol{\theta} \right\|^2
\end{split}
$$

**Equation 4-9: Ridge Regression closed-form solution**

$$
\hat{\boldsymbol{\theta}} = (\mathbf{X}^T \mathbf{X} + \alpha \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y}
$$

**Equation 4-10: Lasso Regression cost function**

$$
\begin{split}
J(\boldsymbol{\theta})
&= \dfrac{1}{m} \sum\limits*{i=1}^{m}{(\boldsymbol{\theta}^T \mathbf{x}^{(i)} - y^{(i)})^2} + \alpha \sum\limits*{i=1}^{n}\left| \theta*i \right| \\
&= \dfrac{1}{m} \left\| \mathbf{X} \boldsymbol{\theta} - \mathbf{y}) \right\|^2 + \alpha \sum\limits*{i=1}^{n} {\left\| \boldsymbol{\theta} \right\|}\_{1}
\end{split}
$$

**Equation 4-11: Lasso Regression subgradient vector**

$$
g(\boldsymbol{\theta}, J) = \nabla_{\boldsymbol{\theta}}\, \text{MSE}(\boldsymbol{\theta}) + \alpha
\begin{pmatrix}
  \operatorname{sign}(\theta_1) \\
  \operatorname{sign}(\theta_2) \\
  \vdots \\
  \operatorname{sign}(\theta_n) \\
\end{pmatrix} \quad \text{where } \operatorname{sign}(\theta_i) =
\begin{cases}
-1 & \text{if } \theta_i < 0 \\
0 & \text{if } \theta_i = 0 \\
+1 & \text{if } \theta_i > 0
\end{cases}
$$
