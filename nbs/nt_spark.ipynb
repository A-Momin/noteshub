{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports:\" data-toc-modified-id=\"Imports:-1\">Imports:</a></span></li><li><span><a href=\"#Creating-the-Spark-Entry-point-(SparkSession)\" data-toc-modified-id=\"Creating-the-Spark-Entry-point-(SparkSession)-2\">Creating the Spark Entry point (SparkSession)</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3\">Data Preparation</a></span></li><li><span><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-4\">Filtering</a></span></li><li><span><a href=\"#How-to-Handle-Missing-Value\" data-toc-modified-id=\"How-to-Handle-Missing-Value-5\">How to Handle Missing Value</a></span></li><li><span><a href=\"#Demo-with-pyspark.sql.functions\" data-toc-modified-id=\"Demo-with-pyspark.sql.functions-6\">Demo with <code>pyspark.sql.functions</code></a></span></li><li><span><a href=\"#Sampling-Data\" data-toc-modified-id=\"Sampling-Data-7\">Sampling Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Experiment-1\" data-toc-modified-id=\"Experiment-1-7.1\">Experiment 1</a></span></li><li><span><a href=\"#Experiment-2\" data-toc-modified-id=\"Experiment-2-7.2\">Experiment 2</a></span></li></ul></li><li><span><a href=\"#Partitioning\" data-toc-modified-id=\"Partitioning-8\">Partitioning</a></span></li><li><span><a href=\"#I/O\" data-toc-modified-id=\"I/O-9\">I/O</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-from-*.parqet-to-*.csv\" data-toc-modified-id=\"Convert-from-*.parqet-to-*.csv-9.1\">Convert from <code>*.parqet</code> to <code>*.csv</code></a></span></li><li><span><a href=\"#Convert-from-*.csv-to-*.parqet\" data-toc-modified-id=\"Convert-from-*.csv-to-*.parqet-9.2\">Convert from <code>*.csv</code> to <code>*.parqet</code></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, asc, desc, lit,\n",
    "    mean, sum, avg, stddev,\n",
    "    count, countDistinct,\n",
    "    format_number, isnan,\n",
    "    asc, desc, mean, \n",
    "    rank, lag, lead,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructField, StructType, LongType, TimestampType,\n",
    "    StringType, IntegerType, \n",
    "    FloatType, BooleanType,\n",
    "    DateType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## \n",
    "# # You might have noticed this code in the screencast.\n",
    "\n",
    "# import findspark\n",
    "# findspark.init('spark-3.3.3-bin-hadoop3')\n",
    "\n",
    "# # The findspark Python module makes it easier to install\n",
    "# # Spark in local mode on your computer. This is convenient\n",
    "# # for practicing Spark syntax locally. \n",
    "# # However, the workspaces already have Spark installed and you do not\n",
    "# # need to use the findspark module\n",
    "\n",
    "# ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Spark 3.1.0](https://spark.apache.org/docs/3.0.1/index.html)\n",
    "- [Spark Python API](https://spark.apache.org/docs/latest/api/python/index.html#)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/reference/index.html#api-reference)\n",
    "- [Transformations and Actions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html#transformations-and-actions)\n",
    "- [Broadcast and Accumulator](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#broadcast-and-accumulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. When running SQL from within another programming language the results will be returned as a `Dataset`/`DataFrame`.\n",
    "- The `Dataset` API is available in Scala and Java. Python does not have the support for the Dataset API.\n",
    "- The `DataFrame` API is available in Scala, Java, Python, and R\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the natively available data types in PySpark:\n",
    "\n",
    "-   **BooleanType**: Represents a boolean value.\n",
    "-   **ByteType**: Represents a byte value.\n",
    "-   **ShortType**: Represents a short integer value.\n",
    "-   **IntegerType**: Represents an integer value.\n",
    "-   **LongType**: Represents a long integer value.\n",
    "-   **FloatType**: Represents a float value.\n",
    "-   **DoubleType**: Represents a double value.\n",
    "-   **DecimalType**: Represents a decimal value.\n",
    "-   **StringType**: Represents a string value.\n",
    "-   **BinaryType**: Represents a binary (byte array) value.\n",
    "-   **DateType**: Represents a date value.\n",
    "-   **TimestampType**: Represents a timestamp value.\n",
    "-   **ArrayType**: Represents an array of values.\n",
    "-   **MapType**: Represents a map of key-value pairs.\n",
    "-   **StructType**: Represents a struct (complex type) with fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Creating the Spark Entry point (SparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/31 17:12:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"auto\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10000000)\n",
    "\n",
    "# Enable eager evaluation for better formatting of the output\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.get(\"spark.sql.sources.bucketing.enabled\")\n",
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "\n",
    "# Disable Broadcast Join\n",
    "spark.conf.set(\"spar.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spar.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/Users/am/mydocs/Software_Development/notes_hub/nbs/spark-warehouse'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.sql.parquet.filterPushDown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ['DATA'] + '/IBM_Data_Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ! head -3 {DATA_DIR}/imports-85.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "column_names = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration',\n",
    "       'num-of-doors', 'body-style', 'drive-wheels', 'engine-location',\n",
    "       'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type',\n",
    "       'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke',\n",
    "       'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',\n",
    "       'highway-mpg', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sqldf = spark.read.csv(DATA_DIR + \"/imports-85.csv/\", header=False).toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sqldf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Alternative way to rename the columns\n",
    "```python\n",
    "columns = sqldf.columns\n",
    "for old_col, new_col in zip(columns, column_names):\n",
    "    sqldf = sqldf.withColumnRenamed(old_col, new_col)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # creates a temporary view against which we can run SQL queries.\n",
    "# df = sqldf.createOrReplaceTempView('auto')\n",
    "# spark.sql(\"SELECT * FROM auto LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sqldf.printSchema()\n",
    "# sqldf.describe()\n",
    "# sqldf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = spark.range(1, 10000, 1, 10).select(col(\"id\"), rand(10).alias(\"Attribute\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMP_PATH = \"/Users/am/mydocs/Software_Development/Databases/RDBMS/sql/schemas/employees.csv\"\n",
    "DEPT_PATH = \"/Users/am/mydocs/Software_Development/Databases/RDBMS/sql/schemas/department.csv\"\n",
    "\n",
    "emp_schema = StructType(fields=[\n",
    "    StructField('employee_id', StringType(), True),\n",
    "    StructField('first_name',  StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('age', FloatType(), True),\n",
    "    StructField('salary', FloatType(), True),\n",
    "    StructField('joining_date', TimestampType(), True),\n",
    "    StructField('department_id', LongType(), True),\n",
    "    StructField('manager_id', LongType(), True)\n",
    "    ]\n",
    ")\n",
    "dept_schema = StructType(fields=[\n",
    "    StructField('department_id', LongType(), True),\n",
    "    StructField('department',  StringType(), True)\n",
    "])\n",
    "\n",
    "emp = spark.read.csv(EMP_PATH, header=False, schema=emp_schema)\n",
    "dept = spark.read.csv(DEPT_PATH, header=False, schema=dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----+-------+-------------------+-------------+----------+\n",
      "|employee_id|first_name|last_name| age| salary|       joining_date|department_id|manager_id|\n",
      "+-----------+----------+---------+----+-------+-------------------+-------------+----------+\n",
      "|          1|     Alice|  Johnson|59.0|70000.0|2000-05-27 16:00:33|            1|      NULL|\n",
      "|          2|      John|    Smith|30.0|60000.0|2001-02-27 16:00:33|            1|         5|\n",
      "|          3|     James|    Smith|25.0|55000.0|2001-01-20 16:00:33|            1|         6|\n",
      "|          4|      Mona|     null|28.0|62000.0|2000-05-27 16:00:33|            1|         7|\n",
      "|          5|      Bill|  Clinton|29.0|54000.0|2024-05-27 16:00:33|            1|         5|\n",
      "|          6|    Hilary|  Clinton|24.0|52000.0|2024-05-27 16:00:33|            1|         5|\n",
      "|          7|       Eva|  Clinton|31.0|63000.0|2024-05-27 16:00:33|            1|         7|\n",
      "|          8|   Charlie|    Brown|27.0|58000.0|2024-05-27 16:00:33|            4|         5|\n",
      "|          9|     Grace|    Brown|33.0|74000.0|2024-05-27 16:00:33|            4|         5|\n",
      "|         10|       Bob| Williams|32.0|65000.0|2024-05-27 16:00:33|            3|         6|\n",
      "|         11|     David| Williams|29.0|    0.0|2024-05-27 16:00:33|            3|         7|\n",
      "|         12|     Frank|     NULL|26.0|61000.0|2024-05-27 16:00:33|            3|         6|\n",
      "|         13|    Nathan| Williams|32.0|70000.0|2024-05-27 16:00:33|            3|         5|\n",
      "|         14|     Henry|Jefferson|34.0|66000.0|2024-05-27 16:00:33|            2|         7|\n",
      "|         15|      Jack|Jefferson|36.0|78000.0|2024-05-27 16:00:33|            2|         7|\n",
      "|         16|     Kathy|Jefferson|38.0|80000.0|2024-05-27 16:00:33|            2|         6|\n",
      "|         17|    Thomas|Jefferson|27.0|67000.0|2000-05-27 16:00:33|            2|         6|\n",
      "|         18|    Olivia|Jefferson|30.0|75000.0|2024-05-27 16:00:33|            2|         6|\n",
      "|         19|     Peter|   Wilson|33.0|77000.0|2024-05-27 16:00:33|            4|         7|\n",
      "|         20|    Andrew|   Wilson|29.0|69000.0|2024-05-27 16:00:33|            4|         5|\n",
      "+-----------+----------+---------+----+-------+-------------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-----------+\n",
      "|department_id| department|\n",
      "+-------------+-----------+\n",
      "|            1|Engineering|\n",
      "|            2|         IT|\n",
      "|            3|         HR|\n",
      "|            4|    Finance|\n",
      "|            5|  Marketing|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = emp\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>salary</th>\n",
       "      <th>age</th>\n",
       "      <th>department</th>\n",
       "      <th>manager_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>John</td>\n",
       "      <td>Smith</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>James</td>\n",
       "      <td>Smith</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mona</td>\n",
       "      <td>null</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Hilary</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Eva</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Brown</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Grace</td>\n",
       "      <td>Brown</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Williams</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>HR</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>David</td>\n",
       "      <td>Williams</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>HR</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Frank</td>\n",
       "      <td>None</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>HR</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Nathan</td>\n",
       "      <td>Williams</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>HR</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Henry</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Olivia</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Peter</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>null</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id first_name  last_name   salary   age   department  manager_id\n",
       "0            1      Alice    Johnson  70000.0  59.0  Engineering         NaN\n",
       "1            2       John      Smith  60000.0  30.0  Engineering         5.0\n",
       "2            3      James      Smith  55000.0  25.0  Engineering         6.0\n",
       "3            4       Mona       null  62000.0  28.0  Engineering         7.0\n",
       "4            5       Bill    Clinton  54000.0  29.0  Engineering         5.0\n",
       "5            6     Hilary    Clinton  52000.0  24.0  Engineering         5.0\n",
       "6            7        Eva    Clinton  63000.0  31.0  Engineering         7.0\n",
       "7            8    Charlie      Brown  58000.0  27.0      Finance         5.0\n",
       "8            9      Grace      Brown  74000.0  33.0      Finance         5.0\n",
       "9           10        Bob   Williams  65000.0  32.0           HR         6.0\n",
       "10          11      David   Williams      0.0  29.0           HR         7.0\n",
       "11          12      Frank       None  61000.0  26.0           HR         6.0\n",
       "12          13     Nathan   Williams  70000.0  32.0           HR         5.0\n",
       "13          14      Henry  Jefferson  66000.0  34.0           IT         7.0\n",
       "14          15       Jack  Jefferson  78000.0  36.0           IT         7.0\n",
       "15          16      Kathy  Jefferson  80000.0  38.0           IT         6.0\n",
       "16          17     Thomas  Jefferson  67000.0  27.0           IT         6.0\n",
       "17          18     Olivia  Jefferson  75000.0  30.0           IT         6.0\n",
       "18          19      Peter     Wilson  77000.0  33.0      Finance         7.0\n",
       "19          20     Andrew     Wilson  69000.0  29.0      Finance         5.0\n",
       "20        null     Thomas  Jefferson      0.0  38.0           IT         6.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inner Join\n",
    "df = emp.join(dept, emp.department_id == dept.department_id, \"inner\").select(\"employee_id\",\"first_name\",\"last_name\",\"salary\",\"age\",\"department\",\"manager_id\")\n",
    "pddf = df.toPandas(); pddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = df.select(\"employee_id\",\"first_name\",\"last_name\",\"age\",\"department\",\"manager_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"PySpark Filtering Examples\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Sample data\n",
    "# data = [\n",
    "#     (\"Alice\", 34, \"2023-01-01\", \"HR\"),\n",
    "#     (\"Bob\", 45, \"2022-12-15\", \"Finance\"),\n",
    "#     (\"Catherine\", 29, \"2023-03-05\", \"HR\"),\n",
    "#     (\"David\", 50, \"2021-07-30\", \"Finance\"),\n",
    "#     (\"Eva\", 40, \"2023-05-22\", \"IT\")\n",
    "# ]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = spark.createDataFrame(data, [\"name\", \"age\", \"join_date\", \"department\"])\n",
    "\n",
    "# # Show the original DataFrame\n",
    "# print(\"Original DataFrame:\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|\n",
      "|          7|       Eva|  Clinton|63000.0|31.0|Engineering|         7|\n",
      "|          9|     Grace|    Brown|74000.0|33.0|    Finance|         5|\n",
      "|         10|       Bob| Williams|65000.0|32.0|         HR|         6|\n",
      "|         13|    Nathan| Williams|70000.0|32.0|         HR|         5|\n",
      "|         14|     Henry|Jefferson|66000.0|34.0|         IT|         7|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|         IT|         7|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|         IT|         6|\n",
      "|         19|     Peter|   Wilson|77000.0|33.0|    Finance|         7|\n",
      "|       null|    Thomas|Jefferson|    0.0|38.0|         IT|         6|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering with filter\n",
    "df.filter(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+----+----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age|department|manager_id|\n",
      "+-----------+----------+---------+-------+----+----------+----------+\n",
      "|         10|       Bob| Williams|65000.0|32.0|        HR|         6|\n",
      "|         11|     David| Williams|    0.0|29.0|        HR|         7|\n",
      "|         12|     Frank|     NULL|61000.0|26.0|        HR|         6|\n",
      "|         13|    Nathan| Williams|70000.0|32.0|        HR|         5|\n",
      "+-----------+----------+---------+-------+----+----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|\n",
      "|          7|       Eva|  Clinton|63000.0|31.0|Engineering|         7|\n",
      "|          9|     Grace|    Brown|74000.0|33.0|    Finance|         5|\n",
      "|         10|       Bob| Williams|65000.0|32.0|         HR|         6|\n",
      "|         13|    Nathan| Williams|70000.0|32.0|         HR|         5|\n",
      "|         14|     Henry|Jefferson|66000.0|34.0|         IT|         7|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|         IT|         7|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|         IT|         6|\n",
      "|         19|     Peter|   Wilson|77000.0|33.0|    Finance|         7|\n",
      "|       null|    Thomas|Jefferson|    0.0|38.0|         IT|         6|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|         IT|         7|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|         IT|         6|\n",
      "|       null|    Thomas|Jefferson|    0.0|38.0|         IT|         6|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n",
      "+-----------+----------+---------+------+---+----------+----------+\n",
      "|employee_id|first_name|last_name|salary|age|department|manager_id|\n",
      "+-----------+----------+---------+------+---+----------+----------+\n",
      "+-----------+----------+---------+------+---+----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|\n",
      "|         14|     Henry|Jefferson|66000.0|34.0|         IT|         7|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|         IT|         7|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|         IT|         6|\n",
      "|         17|    Thomas|Jefferson|67000.0|27.0|         IT|         6|\n",
      "|         18|    Olivia|Jefferson|75000.0|30.0|         IT|         6|\n",
      "|       null|    Thomas|Jefferson|    0.0|38.0|         IT|         6|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age|department|manager_id|\n",
      "+-----------+----------+---------+-------+----+----------+----------+\n",
      "|         10|       Bob| Williams|65000.0|32.0|        HR|         6|\n",
      "|         11|     David| Williams|    0.0|29.0|        HR|         7|\n",
      "|         12|     Frank|     NULL|61000.0|26.0|        HR|         6|\n",
      "|         13|    Nathan| Williams|70000.0|32.0|        HR|         5|\n",
      "|         14|     Henry|Jefferson|66000.0|34.0|        IT|         7|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|        IT|         7|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|        IT|         6|\n",
      "|         17|    Thomas|Jefferson|67000.0|27.0|        IT|         6|\n",
      "|         18|    Olivia|Jefferson|75000.0|30.0|        IT|         6|\n",
      "|       null|    Thomas|Jefferson|    0.0|38.0|        IT|         6|\n",
      "+-----------+----------+---------+-------+----+----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|\n",
      "|         20|    Andrew|   Wilson|69000.0|29.0|    Finance|         5|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|\n",
      "|          7|       Eva|  Clinton|63000.0|31.0|Engineering|         7|\n",
      "|         20|    Andrew|   Wilson|69000.0|29.0|    Finance|         5|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "|          2|      John|    Smith|60000.0|30.0|Engineering|         5|\n",
      "|          7|       Eva|  Clinton|63000.0|31.0|Engineering|         7|\n",
      "|          9|     Grace|    Brown|74000.0|33.0|    Finance|         5|\n",
      "|         10|       Bob| Williams|65000.0|32.0|         HR|         6|\n",
      "|         13|    Nathan| Williams|70000.0|32.0|         HR|         5|\n",
      "|         14|     Henry|Jefferson|66000.0|34.0|         IT|         7|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|         IT|         7|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|         IT|         6|\n",
      "|         18|    Olivia|Jefferson|75000.0|30.0|         IT|         6|\n",
      "|         19|     Peter|   Wilson|77000.0|33.0|    Finance|         7|\n",
      "|       null|    Thomas|Jefferson|    0.0|38.0|         IT|         6|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering with where\n",
    "df.where(df.department == \"HR\").show()\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Filtering with SQL query\n",
    "filtered_df = spark.sql(\"SELECT * FROM employees WHERE age > 30\")\n",
    "filtered_df.show()\n",
    "\n",
    "# Filtering with col\n",
    "df.filter(col(\"age\") > 35).show()\n",
    "\n",
    "# Filtering with AND condition\n",
    "df.filter((df.age > 35) & (df.department == \"HR\")).show()\n",
    "\n",
    "# Filtering with OR condition\n",
    "df.filter((df.age > 35) | (df.department == \"IT\")).show()\n",
    "\n",
    "# Filtering with isin\n",
    "df.filter(df.department.isin(\"HR\", \"IT\")).show()\n",
    "\n",
    "# Filtering with like\n",
    "df.filter(df.first_name.like(\"A%\")).show()\n",
    "\n",
    "# Filtering with rlike (regular expression)\n",
    "df.filter(df.first_name.rlike(\"^[AE].*\")).show()\n",
    "\n",
    "# Filtering with between\n",
    "df.filter(df.age.between(30, 40)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+----------+\n",
      "| name| age| join_date|department|\n",
      "+-----+----+----------+----------+\n",
      "|Alice|NULL|2023-01-01|        HR|\n",
      "+-----+----+----------+----------+\n",
      "\n",
      "+---------+----+----------+----------+\n",
      "|     name| age| join_date|department|\n",
      "+---------+----+----------+----------+\n",
      "|    Alice|NULL|2023-01-01|        HR|\n",
      "|      Bob|  45|2022-12-15|   Finance|\n",
      "|Catherine|  29|2023-03-05|        HR|\n",
      "|      Eva|  40|2023-05-22|        IT|\n",
      "+---------+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data with null values\n",
    "data_with_nulls = [\n",
    "    (\"Alice\", None, \"2023-01-01\", \"HR\"),\n",
    "    (\"Bob\", 45, \"2022-12-15\", \"Finance\"),\n",
    "    (\"Catherine\", 29, \"2023-03-05\", \"HR\"),\n",
    "    (\"David\", 50, \"2021-07-30\", None),\n",
    "    (\"Eva\", 40, \"2023-05-22\", \"IT\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_nulls = spark.createDataFrame(data_with_nulls, [\"name\", \"age\", \"join_date\", \"department\"])\n",
    "\n",
    "# Filtering with isNull\n",
    "df_nulls.filter(df_nulls.age.isNull()).show()\n",
    "\n",
    "# Filtering with isNotNull\n",
    "df_nulls.filter(df_nulls.department.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+----+-----------+----------+--------+\n",
      "|employee_id|first_name|last_name| salary| age| department|manager_id|AgeGroup|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+--------+\n",
      "|          1|     Alice|  Johnson|70000.0|59.0|Engineering|      NULL|  senior|\n",
      "|          2|      John|    Smith|60000.0|30.0|Engineering|         5|  middle|\n",
      "|          3|     James|    Smith|55000.0|25.0|Engineering|         6|   young|\n",
      "|          4|      Mona|     null|62000.0|28.0|Engineering|         7|   young|\n",
      "|          5|      Bill|  Clinton|54000.0|29.0|Engineering|         5|   young|\n",
      "|          6|    Hilary|  Clinton|52000.0|24.0|Engineering|         5|   young|\n",
      "|          7|       Eva|  Clinton|63000.0|31.0|Engineering|         7|  middle|\n",
      "|          8|   Charlie|    Brown|58000.0|27.0|    Finance|         5|   young|\n",
      "|          9|     Grace|    Brown|74000.0|33.0|    Finance|         5|  middle|\n",
      "|         10|       Bob| Williams|65000.0|32.0|         HR|         6|  middle|\n",
      "|         11|     David| Williams|    0.0|29.0|         HR|         7|   young|\n",
      "|         12|     Frank|     NULL|61000.0|26.0|         HR|         6|   young|\n",
      "|         13|    Nathan| Williams|70000.0|32.0|         HR|         5|  middle|\n",
      "|         14|     Henry|Jefferson|66000.0|34.0|         IT|         7|  middle|\n",
      "|         15|      Jack|Jefferson|78000.0|36.0|         IT|         7|  middle|\n",
      "|         16|     Kathy|Jefferson|80000.0|38.0|         IT|         6|  middle|\n",
      "|         17|    Thomas|Jefferson|67000.0|27.0|         IT|         6|   young|\n",
      "|         18|    Olivia|Jefferson|75000.0|30.0|         IT|         6|  middle|\n",
      "|         19|     Peter|   Wilson|77000.0|33.0|    Finance|         7|  middle|\n",
      "|         20|    Andrew|   Wilson|69000.0|29.0|    Finance|         5|   young|\n",
      "+-----------+----------+---------+-------+----+-----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    'AgeGroup',\n",
    "    when(df['Age']<20, 'junior')\n",
    "    .when((df['Age']>=20) & (df['Age']<30), 'young')\n",
    "    .when((df['Age']>=30) & (df['Age']<55), 'middle')\n",
    "    .when((df['Age']>=55) & (df['Age']<70), 'senior')\n",
    "    .otherwise('elderly')\n",
    "); df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # SQL \n",
    "# query = \"\"\"\n",
    "# SELECT\n",
    "#     Age,\n",
    "#     CASE\n",
    "#         WHEN Age < 20 THEN 'junior'\n",
    "#         WHEN (Age >=20) AND (Age<30) THEN 'young'\n",
    "#         WHEN Age BETWEEN 30 AND 55 THEN 'middle' -- Age==30 and Age==55 are classified as 'middle'\n",
    "#         WHEN Age>=55 AND Age<70 THEN 'senior'\n",
    "#         ELSE 'elderly'\n",
    "#     END AS AgeGroup\n",
    "# FROM\n",
    "#     df\n",
    "# \"\"\"\n",
    "\n",
    "# spark.sql(query).show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GROUP_BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "| department|      avg(salary)|\n",
      "+-----------+-----------------+\n",
      "|Engineering|59428.57142857143|\n",
      "|         HR|          49000.0|\n",
      "|    Finance|          69500.0|\n",
      "|         IT|          61000.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').agg(avg('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+--------+---------------+\n",
      "| department| salary|sum(salary)|sum(age)|sum(manager_id)|\n",
      "+-----------+-------+-----------+--------+---------------+\n",
      "|Engineering|52000.0|    52000.0|    24.0|              5|\n",
      "|         IT|78000.0|    78000.0|    36.0|              7|\n",
      "|    Finance|74000.0|    74000.0|    33.0|              5|\n",
      "|         HR|70000.0|    70000.0|    32.0|              5|\n",
      "|Engineering|55000.0|    55000.0|    25.0|              6|\n",
      "|    Finance|77000.0|    77000.0|    33.0|              7|\n",
      "|Engineering|60000.0|    60000.0|    30.0|              5|\n",
      "|         IT|80000.0|    80000.0|    38.0|              6|\n",
      "|Engineering|54000.0|    54000.0|    29.0|              5|\n",
      "|Engineering|70000.0|    70000.0|    59.0|           NULL|\n",
      "|         HR|61000.0|    61000.0|    26.0|              6|\n",
      "|    Finance|69000.0|    69000.0|    29.0|              5|\n",
      "|Engineering|62000.0|    62000.0|    28.0|              7|\n",
      "|    Finance|58000.0|    58000.0|    27.0|              5|\n",
      "|         IT|66000.0|    66000.0|    34.0|              7|\n",
      "|         IT|67000.0|    67000.0|    27.0|              6|\n",
      "|         IT|    0.0|        0.0|    38.0|              6|\n",
      "|         IT|75000.0|    75000.0|    30.0|              6|\n",
      "|         HR|    0.0|        0.0|    29.0|              7|\n",
      "|         HR|65000.0|    65000.0|    32.0|              6|\n",
      "+-----------+-------+-----------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by multiple conditions\n",
    "train_df.groupBy('department', 'salary').count().sort(asc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average values\n",
    "train_df.groupby('HomePlanet').mean('TotalBill').show()\n",
    "\n",
    "# Combination with filter() function\n",
    "train_df.groupBy('HomePlanet').mean('TotalBill') \\\n",
    "        .filter(col('avg(TotalBill)') >= 1000).show()\n",
    "\n",
    "# mean() function calls agg(avg()) function. After agg() function, you can use alias() function for renaming.\n",
    "train_df.groupBy('HomePlanet').agg(avg('TotalBill') \\\n",
    "        .alias('mean_total_bill')).filter(col('mean_total_bill') >= 1000) \\\n",
    "        .sort(desc('mean_total_bill')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple conditions and aggregate multiple functions\n",
    "train_df.groupBy('HomePlanet').agg(avg('TotalBill').alias('avg_total_bill'), \\\n",
    "                                   stddev('TotalBill').alias('stddev_total_bill')) \\\n",
    "                                   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Table \n",
    "# You can get aggregation group members as a header by pivottable.\n",
    "train_df.groupBy('HomePlanet').agg(count('VIP').alias('VIPCount')).show()\n",
    "train_df.groupby('HomePlanet').pivot('VIP').count().show()\n",
    "train_df.groupby('HomePlanet').pivot('VIP').max('TotalBill').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    HomePlanet\n",
    "    , COUNT(*) AS Count\n",
    "    , COUNT(DISTINCT PassengerId) as TotalIDs\n",
    "    , AVG(TotalBill) AS mean_total_bill\n",
    "    , MAX(TotalBill) AS max_total_bill\n",
    "    , SUM(TotalBill) AS sum_of_total_bill\n",
    "FROM\n",
    "    train_df\n",
    "GROUP BY\n",
    "    HomePlanet\n",
    "HAVING\n",
    "    mean_total_bill > 1000\n",
    "ORDER BY\n",
    "    mean_total_bill DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple conditions\n",
    "train_df.groupBy('HomePlanet', 'VIP').count().sort(asc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average values\n",
    "train_df.groupby('HomePlanet').mean('TotalBill').show()\n",
    "\n",
    "# Combination with filter() function\n",
    "train_df.groupBy('HomePlanet').mean('TotalBill') \\\n",
    "        .filter(col('avg(TotalBill)') >= 1000).show()\n",
    "\n",
    "# mean() function calls agg(avg()) function. After agg() function, you can use alias() function for renaming.\n",
    "train_df.groupBy('HomePlanet').agg(avg('TotalBill') \\\n",
    "        .alias('mean_total_bill')).filter(col('mean_total_bill') >= 1000) \\\n",
    "        .sort(desc('mean_total_bill')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple conditions and aggregate multiple functions\n",
    "train_df.groupBy('HomePlanet').agg(avg('TotalBill').alias('avg_total_bill'), \\\n",
    "                                   stddev('TotalBill').alias('stddev_total_bill')) \\\n",
    "                                   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Table \n",
    "# You can get aggregation group members as a header by pivottable.\n",
    "train_df.groupBy('HomePlanet').agg(count('VIP').alias('VIPCount')).show()\n",
    "train_df.groupby('HomePlanet').pivot('VIP').count().show()\n",
    "train_df.groupby('HomePlanet').pivot('VIP').max('TotalBill').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    HomePlanet\n",
    "    , COUNT(*) AS Count\n",
    "    , COUNT(DISTINCT PassengerId) as TotalIDs\n",
    "    , AVG(TotalBill) AS mean_total_bill\n",
    "    , MAX(TotalBill) AS max_total_bill\n",
    "    , SUM(TotalBill) AS sum_of_total_bill\n",
    "FROM\n",
    "    train_df\n",
    "GROUP BY\n",
    "    HomePlanet\n",
    "HAVING\n",
    "    mean_total_bill > 1000\n",
    "ORDER BY\n",
    "    mean_total_bill DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### How to Handle Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Replace all nulls with a specific value\n",
    "df = df.fillna({\n",
    "    'first_name': 'Tom',\n",
    "    'age': 0,\n",
    "})\n",
    "\n",
    "# Take the first value that is not null\n",
    "df = df.withColumn('last_name', F.coalesce(df.last_name, df.surname, F.lit('N/A')))\n",
    "\n",
    "# Drop duplicate rows in a dataset (distinct)\n",
    "df = df.dropDuplicates() # or\n",
    "df = df.distinct()\n",
    "\n",
    "# Drop duplicate rows, but consider only specific columns\n",
    "df = df.dropDuplicates(['name', 'height'])\n",
    "\n",
    "# Replace empty strings with null (leave out subset keyword arg to replace in all columns)\n",
    "df = df.replace({\"\": None}, subset=[\"name\"])\n",
    "\n",
    "# Convert Python/PySpark/NumPy NaN operator to null\n",
    "df = df.replace(float(\"nan\"), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"HandleMissingData\").getOrCreate()\n",
    "\n",
    "# Sample data with missing values\n",
    "data = [\n",
    "    (1, \"John\", None),\n",
    "    (2, None, 5000),\n",
    "    (3, \"Sara\", 4500),\n",
    "    (None, \"David\", None),\n",
    "    (5, \"Mike\", 5500)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with any null values\n",
    "df_dropped_any = df.na.drop()\n",
    "df_dropped_any.show()\n",
    "\n",
    "df_dropped_any2 = df.dropna('any')\n",
    "df_dropped_any2.show()\n",
    "\n",
    "# Drop rows with all null values\n",
    "df_dropped_all = df.na.drop(how='all')\n",
    "df_dropped_all.show()\n",
    "\n",
    "df_dropped_all2 = df.dropna('all')\n",
    "\n",
    "# Drop rows with null values in specific columns\n",
    "df_dropped_subset = df.na.drop(subset=['name', 'salary'])\n",
    "df_dropped_subset.show()\n",
    "\n",
    "# Fill all null values with a specified value\n",
    "df_filled_all = df.na.fill(\"Unknown\")\n",
    "df_filled_all.show()\n",
    "\n",
    "\n",
    "# Fill null values in specific columns\n",
    "df_filled_subset = df.na.fill({\"name\": \"Unknown\", \"salary\": 0})\n",
    "df_filled_subset.show()\n",
    "\n",
    "df_filled_subset = df.fillna('unknown', subset=['name'])\n",
    "\n",
    "# Replace specific values\n",
    "# df_replaced = df.na.replace({None: \"Unknown\"})\n",
    "# df_replaced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Using SQL to handle missing data\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "filled_df_sql = spark.sql(\"\"\"\n",
    "SELECT id,\n",
    "       COALESCE(name, 'Unknown') as name,\n",
    "       COALESCE(salary, 0) as salary\n",
    "FROM people\n",
    "\"\"\")\n",
    "filled_df_sql.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# coalesce and na.fill\n",
    "df_nulls.withColumn(\"name_filled\", coalesce(\"name\", lit(\"Unknown\"))) \\\n",
    "    .withColumn(\"salary_filled\", coalesce(\"salary\", lit(0))) \\\n",
    "    .na.fill({\"name\": \"Unknown\", \"salary\": 0}) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Demo with `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 17:17:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------------+\n",
      "|     name|age|         city|\n",
      "+---------+---+-------------+\n",
      "|    Alice| 34|     New York|\n",
      "|      Bob| 45|San Francisco|\n",
      "|Catherine| 29|      Chicago|\n",
      "|    David| 50|     New York|\n",
      "|      Eva| 40|San Francisco|\n",
      "+---------+---+-------------+\n",
      "\n",
      "Transformed DataFrame:\n",
      "+---------+---+-------------+-------+---------+-------------------+-----------+-----------+----------+----------+------------+--------------+----+-----+---+\n",
      "|     name|age|         city|country|age_group|          name_city|name_substr|name_length|name_upper|name_lower|current_date|formatted_date|year|month|day|\n",
      "+---------+---+-------------+-------+---------+-------------------+-----------+-----------+----------+----------+------------+--------------+----+-----+---+\n",
      "|    Alice| 34|     New York|    USA|    Young|   Alice - New York|        Ali|          5|     ALICE|     alice|  2024-05-31|    05/31/2024|2024|    5| 31|\n",
      "|      Bob| 45|San Francisco|    USA|      Old|Bob - San Francisco|        Bob|          3|       BOB|       bob|  2024-05-31|    05/31/2024|2024|    5| 31|\n",
      "|Catherine| 29|      Chicago|    USA|    Young|Catherine - Chicago|        Cat|          9| CATHERINE| catherine|  2024-05-31|    05/31/2024|2024|    5| 31|\n",
      "|    David| 50|     New York|    USA|      Old|   David - New York|        Dav|          5|     DAVID|     david|  2024-05-31|    05/31/2024|2024|    5| 31|\n",
      "|      Eva| 40|San Francisco|    USA|      Old|Eva - San Francisco|        Eva|          3|       EVA|       eva|  2024-05-31|    05/31/2024|2024|    5| 31|\n",
      "+---------+---+-------------+-------+---------+-------------------+-----------+-----------+----------+----------+------------+--------------+----+-----+---+\n",
      "\n",
      "DataFrame with Rounded Scores:\n",
      "+---------+------+-------------+--------------+\n",
      "|     name| score|score_rounded|score_brounded|\n",
      "+---------+------+-------------+--------------+\n",
      "|    Alice|34.567|         34.6|          34.6|\n",
      "|      Bob|45.123|         45.1|          45.1|\n",
      "|Catherine|29.987|         30.0|          30.0|\n",
      "|    David|50.456|         50.5|          50.5|\n",
      "|      Eva|40.789|         40.8|          40.8|\n",
      "+---------+------+-------------+--------------+\n",
      "\n",
      "Aggregated DataFrame:\n",
      "+-------------+--------------+--------------+\n",
      "|city         |names_list    |names_set     |\n",
      "+-------------+--------------+--------------+\n",
      "|New York     |[Alice, David]|[Alice, David]|\n",
      "|San Francisco|[Bob, Eva]    |[Eva, Bob]    |\n",
      "|Chicago      |[Catherine]   |[Catherine]   |\n",
      "+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, concat, concat_ws, substring, length, \n",
    "    upper, lower, round, bround, date_format, current_date, year, month, \n",
    "    dayofmonth, collect_list, collect_set\n",
    ")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark SQL Functions Examples\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 34, \"New York\"),\n",
    "    (\"Bob\", 45, \"San Francisco\"),\n",
    "    (\"Catherine\", 29, \"Chicago\"),\n",
    "    (\"David\", 50, \"New York\"),\n",
    "    (\"Eva\", 40, \"San Francisco\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Demonstrating various functions\n",
    "df = df.withColumn(\"country\", lit(\"USA\"))\n",
    "df = df.withColumn(\"age_group\", when(col(\"age\") < 40, \"Young\").otherwise(\"Old\"))\n",
    "df = df.withColumn(\"name_city\", concat_ws(\" - \", col(\"name\"), col(\"city\")))\n",
    "df = df.withColumn(\"name_substr\", substring(col(\"name\"), 1, 3))\n",
    "df = df.withColumn(\"name_length\", length(col(\"name\")))\n",
    "df = df.withColumn(\"name_upper\", upper(col(\"name\"))).withColumn(\"name_lower\", lower(col(\"name\")))\n",
    "\n",
    "# Adding a current date column for demonstration\n",
    "df = df.withColumn(\"current_date\", current_date())\n",
    "df = df.withColumn(\"formatted_date\", date_format(col(\"current_date\"), \"MM/dd/yyyy\"))\n",
    "df = df.withColumn(\"year\", year(col(\"current_date\"))).withColumn(\"month\", month(col(\"current_date\"))).withColumn(\"day\", dayofmonth(col(\"current_date\")))\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "print(\"Transformed DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Creating a DataFrame with a float column for demonstration\n",
    "data = [(\"Alice\", 34.567), (\"Bob\", 45.123), (\"Catherine\", 29.987), (\"David\", 50.456), (\"Eva\", 40.789)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"score\"])\n",
    "\n",
    "df = df.withColumn(\"score_rounded\", round(col(\"score\"), 1)).withColumn(\"score_brounded\", bround(col(\"score\"), 1))\n",
    "\n",
    "# Show the DataFrame with rounded scores\n",
    "print(\"DataFrame with Rounded Scores:\")\n",
    "df.show()\n",
    "\n",
    "# Group by city and aggregate names\n",
    "df = spark.createDataFrame([\n",
    "    (\"Alice\", 34, \"New York\"),\n",
    "    (\"Bob\", 45, \"San Francisco\"),\n",
    "    (\"Catherine\", 29, \"Chicago\"),\n",
    "    (\"David\", 50, \"New York\"),\n",
    "    (\"Eva\", 40, \"San Francisco\")\n",
    "], [\"name\", \"age\", \"city\"])\n",
    "\n",
    "df = df.groupBy(\"city\").agg(collect_list(\"name\").alias(\"names_list\"), collect_set(\"name\").alias(\"names_set\"))\n",
    "\n",
    "# Show the aggregated DataFrame\n",
    "print(\"Aggregated DataFrame:\")\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, date_add, date_sub, datediff, months_between, current_date, \n",
    "    year, month, dayofmonth, dayofweek, dayofyear, weekofyear, hour, minute, \n",
    "    second, current_timestamp, unix_timestamp, coalesce, split, array_contains\n",
    ")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark SQL Functions Additional Examples\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", 34, \"2023-01-01\"),\n",
    "    (\"Bob\", 45, \"2022-12-15\"),\n",
    "    (\"Catherine\", 29, \"2023-03-05\"),\n",
    "    (\"David\", 50, \"2021-07-30\"),\n",
    "    (\"Eva\", 40, \"2023-05-22\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"join_date\"])\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# date_add and date_sub\n",
    "df.withColumn(\"date_plus_10\", date_add(\"join_date\", 10)) \\\n",
    "  .withColumn(\"date_minus_10\", date_sub(\"join_date\", 10)) \\\n",
    "  .show()\n",
    "\n",
    "# datediff and months_between\n",
    "df.withColumn(\"days_since_join\", datediff(current_date(), \"join_date\")) \\\n",
    "  .withColumn(\"months_since_join\", months_between(current_date(), \"join_date\")) \\\n",
    "  .show()\n",
    "\n",
    "# year, month, dayofmonth, dayofweek, dayofyear, weekofyear\n",
    "df.withColumn(\"year\", year(\"join_date\")) \\\n",
    "  .withColumn(\"month\", month(\"join_date\")) \\\n",
    "  .withColumn(\"day\", dayofmonth(\"join_date\")) \\\n",
    "  .withColumn(\"day_of_week\", dayofweek(\"join_date\")) \\\n",
    "  .withColumn(\"day_of_year\", dayofyear(\"join_date\")) \\\n",
    "  .withColumn(\"week_of_year\", weekofyear(\"join_date\")) \\\n",
    "  .show()\n",
    "\n",
    "# Sample data with timestamp\n",
    "data_with_time = [\n",
    "    (\"Alice\", 34, \"2023-01-01 12:34:56\"),\n",
    "    (\"Bob\", 45, \"2022-12-15 14:20:30\"),\n",
    "    (\"Catherine\", 29, \"2023-03-05 08:45:15\"),\n",
    "    (\"David\", 50, \"2021-07-30 19:50:40\"),\n",
    "    (\"Eva\", 40, \"2023-05-22 06:25:10\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_time = spark.createDataFrame(data_with_time, [\"name\", \"age\", \"join_time\"])\n",
    "\n",
    "# hour, minute, second\n",
    "df_time.withColumn(\"hour\", hour(\"join_time\")) \\\n",
    "      .withColumn(\"minute\", minute(\"join_time\")) \\\n",
    "      .withColumn(\"second\", second(\"join_time\")) \\\n",
    "      .show()\n",
    "\n",
    "# current_timestamp and unix_timestamp\n",
    "df.withColumn(\"current_ts\", current_timestamp()) \\\n",
    "  .withColumn(\"unix_ts\", unix_timestamp(\"join_date\")) \\\n",
    "  .show()\n",
    "\n",
    "# Sample data with null values\n",
    "data_with_nulls = [\n",
    "    (\"Alice\", None),\n",
    "    (None, 45),\n",
    "    (\"Catherine\", 29),\n",
    "    (\"David\", None),\n",
    "    (\"Eva\", 40)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_nulls = spark.createDataFrame(data_with_nulls, [\"name\", \"age\"])\n",
    "\n",
    "# coalesce and na.fill\n",
    "df_nulls.withColumn(\"name_filled\", coalesce(\"name\", lit(\"Unknown\"))) \\\n",
    "        .withColumn(\"age_filled\", coalesce(\"age\", lit(0))) \\\n",
    "        .na.fill({\"name\": \"Unknown\", \"age\": 0}) \\\n",
    "        .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Construct a new dynamic column\n",
    "df = df.withColumn('full_name', F.when(\n",
    "    (df.fname.isNotNull() & df.lname.isNotNull()), F.concat(df.fname, df.lname)\n",
    ").otherwise(F.lit('N/A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Demo how to use 'collect_list() and explode() functions'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 11:16:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name|subject|\n",
      "+-------+-------+\n",
      "|  Alice|   Math|\n",
      "|  Alice|Science|\n",
      "|    Bob|   Math|\n",
      "|    Bob|English|\n",
      "|Charlie|   Math|\n",
      "|Charlie|Science|\n",
      "|Charlie|History|\n",
      "+-------+-------+\n",
      "\n",
      "DataFrame after collect_list():\n",
      "+-------+------------------------+\n",
      "|name   |subjects                |\n",
      "+-------+------------------------+\n",
      "|Alice  |[Math, Science]         |\n",
      "|Bob    |[Math, English]         |\n",
      "|Charlie|[Math, Science, History]|\n",
      "+-------+------------------------+\n",
      "\n",
      "DataFrame after explode():\n",
      "+-------+-------+\n",
      "|   name|subject|\n",
      "+-------+-------+\n",
      "|  Alice|   Math|\n",
      "|  Alice|Science|\n",
      "|    Bob|   Math|\n",
      "|    Bob|English|\n",
      "|Charlie|   Math|\n",
      "|Charlie|Science|\n",
      "|Charlie|History|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list, explode\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Collect List and Explode Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"Math\"),\n",
    "    (\"Alice\", \"Science\"),\n",
    "    (\"Bob\", \"Math\"),\n",
    "    (\"Bob\", \"English\"),\n",
    "    (\"Charlie\", \"Math\"),\n",
    "    (\"Charlie\", \"Science\"),\n",
    "    (\"Charlie\", \"History\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"subject\"])\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Group by name and collect subjects into a list\n",
    "collected_df = df.groupBy(\"name\").agg(collect_list(\"subject\").alias(\"subjects\"))\n",
    "\n",
    "# Show the DataFrame with collected lists\n",
    "print(\"DataFrame after collect_list():\")\n",
    "collected_df.show(truncate=False)\n",
    "\n",
    "# Explode the list of subjects into individual rows\n",
    "exploded_df = collected_df.select(\"name\", explode(\"subjects\").alias(\"subject\"))\n",
    "\n",
    "# Show the DataFrame with exploded lists\n",
    "print(\"DataFrame after explode():\")\n",
    "exploded_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sampling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ['DATA'] + '/Spark_Experiments'\n",
    "transactions_file = DATA_DIR + \"/TNX.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "chunk_size = 1000000  # Adjust based on available memory\n",
    "sample = pd.DataFrame()\n",
    "\n",
    "df = pd.read_csv(transactions_file)\n",
    "\n",
    "for chunk in pd.read_csv(transactions_file, chunksize=chunk_size):\n",
    "    chunk_sample = chunk.sample(n=min(sample_size, len(chunk)), random_state=1)\n",
    "    sample = pd.concat([sample, chunk_sample], axis=0)\n",
    "    sample = sample.sample(n=min(sample_size, len(sample)), random_state=1)\n",
    "\n",
    "    if len(sample) > sample_size:\n",
    "        sample = sample.sample(n=sample_size, random_state=1)\n",
    "\n",
    "sample.to_csv(DATA_DIR + '/sampled_tnx.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/am/Desktop/endomondoHR_proper.json',\n",
       " '/Users/am/DATA/endomondoHR_proper.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = os.environ['DATA']\n",
    "# Define paths\n",
    "input_parquet_path = os.environ['HOME'] + '/Desktop/endomondoHR_proper.json'\n",
    "# input_parquet_path = DATA_DIR + '/sparkify_log_small.json'\n",
    "# output_csv_path = DATA_DIR + '/sparkify_log_small_test.json'\n",
    "output_csv_path = DATA_DIR + '/endomondoHR_proper.json'\n",
    "input_parquet_path, output_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/27 21:57:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_parquet_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParquet Sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.shuffle.partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Read the Parquet file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(\u001b[43minput_parquet_path\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROPMALFORMED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get the total number of records\u001b[39;00m\n\u001b[1;32m     15\u001b[0m total_records \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_parquet_path' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Parquet Sampling\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.json(input_parquet_path, mode=\"DROPMALFORMED\")\n",
    "\n",
    "# Get the total number of records\n",
    "total_records = df.count()\n",
    "\n",
    "# Calculate the fraction to sample approximately 10,000 records\n",
    "sample_fraction = 10000 / total_records\n",
    "\n",
    "# Sample the DataFrame\n",
    "sampled_df = df.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "# Ensure exactly 10,000 records by limiting after sampling\n",
    "sampled_df = sampled_df.limit(10000)\n",
    "\n",
    "# Write the sampled records to a CSV file\n",
    "sampled_df.write.mode('overwrite').option(\"header\", \"true\").json(output_csv_path)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "## Write Partitioned Data\n",
    "df.write.partitionBy(\"column\").parquet(\"path/to/output\")\n",
    "\n",
    "df_transactions.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").csv(\"TNX_test.csv\")\n",
    "\n",
    "df_transactions.repartition(5).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/TNX_test.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "(\n",
    "    df\n",
    "    .repartition(3)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"listen_date\")\n",
    "    .parquet(DATA_DIR + \"/partitioning/partitioned/listening_activity_pt_4\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## Read CSV Files\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "## Read JSON Files\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "## Read Parquet Files\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "\n",
    "## Read ORC Files\n",
    "df = spark.read.orc(\"path/to/file.orc\")\n",
    "\n",
    "## Read Text Files\n",
    "df = spark.read.text(\"path/to/file.txt\")\n",
    "\n",
    "## Read Avro Files\n",
    "df = spark.read.format(\"avro\").load(\"path/to/file.avro\")\n",
    "\n",
    "## Read Delta Lake\n",
    "df = spark.read.format(\"delta\").load(\"path/to/delta/table\")\n",
    "\n",
    "## Read JDBC/ODBC\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/db\").option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").load()\n",
    "\n",
    "## Read Other Formats\n",
    "df = spark.read.format(\"custom_format\").load(\"path/to/source\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.write.save(out_path, format=\"csv\", header=True)\n",
    "\n",
    "## Write CSV Files\n",
    "df.write.csv(\"path/to/output.csv\", header=True)\n",
    "\n",
    "## Write JSON Files\n",
    "df.write.json(\"path/to/output.json\")\n",
    "\n",
    "## Write Parquet Files\n",
    "df.write.parquet(\"path/to/output.parquet\")\n",
    "\n",
    "## Write ORC Files\n",
    "df.write.orc(\"path/to/output.orc\")\n",
    "\n",
    "## Write Text Files\n",
    "df.write.text(\"path/to/output.txt\")\n",
    "\n",
    "## Write Avro Files\n",
    "df.write.format(\"avro\").save(\"path/to/output.avro\")\n",
    "\n",
    "## Write Delta Lake\n",
    "df.write.format(\"delta\").save(\"path/to/delta/output\")\n",
    "\n",
    "## Write JDBC/ODBC\n",
    "df.write.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/db\").option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").save()\n",
    "\n",
    "## Write Hive Tables\n",
    "df.write.saveAsTable(\"hive_table\")\n",
    "\n",
    "df.write.format(\"parquet\").saveAsTable(\"non_bucketed_table\")\n",
    "\n",
    "## Write Partitioned Data\n",
    "df.write.partitionBy(\"column\").parquet(\"path/to/output\")\n",
    "\n",
    "df_transactions.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").csv(\"TNX_test.csv\")\n",
    "\n",
    "df_transactions.repartition(5).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/TNX_test.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(spark.read.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Option 1:\n",
    "\n",
    "column_names = [\"column1\", \"column2\", \"column3\"]\n",
    "\n",
    "# Read the CSV file with header\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\", header=True, inferSchema=True).toDF(*new_column_names)\n",
    "\n",
    "Option 2:\n",
    "\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\", header=True, inferSchema=True).\n",
    "\n",
    "# Rename columns\n",
    "for idx, new_name in enumerate(new_column_names):\n",
    "    df = df.withColumnRenamed(f\"_c{idx}\", new_name)\n",
    "\n",
    "Option 3:\n",
    "\n",
    "# Define the column names\n",
    "column_names = \"column1,column2,column3\"\n",
    "\n",
    "# Read the CSV file with specified column names\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"delimiter\", \",\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .schema(column_names) \\\n",
    "               .csv(\"path/to/csvfile.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b style=\"color:magenta\">How to read data from MySQL in Apache Spark?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **Download the MySQL JDBC Driver**:\n",
    "\n",
    "You can download the MySQL Connector/J (JDBC driver) from the official MySQL website: MySQL Connector/J.\n",
    "Choose the version that matches your environment (e.g., mysql-connector-java-8.0.30.jar).\n",
    "\n",
    "- **Ensure the Driver is Available to Spark**:\n",
    "\n",
    "Place the downloaded mysql-connector-java-8.0.xx.jar file in a directory accessible by your Spark environment.\n",
    "Note the full path to this JAR file.\n",
    "\n",
    "- **Modify Your Spark Submit Command**:\n",
    "\n",
    "    - Use the `--jars` option to include the MySQL JDBC driver when you submit your Spark job.\n",
    "    - `spark-submit --jars /path/to/mysql-connector-java-8.0.xx.jar scripts/read_mysql.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC URL format: jdbc:mysql://<host>:<port>/<database>\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/interview_questions\"\n",
    "\n",
    "# Connection properties\n",
    "connection_properties = {\n",
    "    \"user\": \"Shah\",\n",
    "    \"password\": \"shah711409\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL Spark Integration\") \\\n",
    "    .config(\"spark.jars\", \"/path/to/mysql-connector-java-8.0.xx.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Reading table from MySQL\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"Employee\", properties=connection_properties)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Convert from `*.parqet` to `*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ['DATA'] + '/Spark_Experiments'\n",
    "transactions_file_pq = DATA_DIR + \"/transactions.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transactions.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").csv(DATA_DIR + \"/TNX_test.csv\")\n",
    "# df_transactions.repartition(5).write.mode(\"overwrite\").option(\"header\", \"true\").csv(DATA_DIR + \"/TNX_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|  amt|         city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n",
      "|CB4ONXHMAX|2012-05-01|      null|TV9Z6GK6TGNU830|2017-10-05|2017|   10|  5|    Groceries|37.01|san_francisco|\n",
      "|CFO5OH0CZ2|2013-02-01|      null|TV24915NS0DVU5O|2014-02-05|2014|    2|  5| Motor/Travel| 6.18|       denver|\n",
      "|C0YDPQWPBJ|2012-01-01|      null|TPLK255YSER4EAT|2016-09-23|2016|    9| 23|Entertainment|21.15| philadelphia|\n",
      "|C0YDPQWPBJ|2011-05-01|2020-09-01|T6GASJ61JA491KC|2015-08-06|2015|    8|  6| Motor/Travel|12.64|      chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2019-05-01|TYK1IXM9VOV6OXD|2012-02-19|2012|    2| 19| Motor/Travel| 65.2|     portland|\n",
      "|C0YDPQWPBJ|2012-05-01|2020-07-01|TEW93GN8XIHW3KP|2019-10-19|2019|   10| 19|    Groceries| 6.23|      chicago|\n",
      "|C7FDFXHBKI|2012-07-01|      null|TM9IGUZTFRV9JL9|2016-08-18|2016|    8| 18|Entertainment|  5.8|      seattle|\n",
      "|C0YDPQWPBJ|2011-05-01|2020-10-01|T26UMWDD80V5T1Z|2013-12-04|2013|   12|  4|    Groceries|54.27|       boston|\n",
      "|CLFK0UN2CV|2013-02-01|      null|T1XLEL42ZMWSYJQ|2013-04-02|2013|    4|  2|Entertainment| 3.98|     new_york|\n",
      "|C0YDPQWPBJ|2012-01-01|      null|TNUD1CB39TZH5DR|2016-03-09|2016|    3|  9|Entertainment| 6.42|     new_york|\n",
      "|C43IGTHLFE|2011-08-01|2020-04-01|TOQBCRWFWDOJJYQ|2015-03-12|2015|    3| 12|Entertainment| 3.57|     new_york|\n",
      "|CJED2CZSVZ|2010-06-01|2018-09-01|TVBH0WMSC7Y4P0Z|2016-05-19|2016|    5| 19|Entertainment|13.43|       boston|\n",
      "|CSQO4JWWMS|2012-11-01|      null|T4U8HYK7NZRF0W6|2015-02-08|2015|    2|  8|Entertainment| 5.56|       boston|\n",
      "|C0YDPQWPBJ|2010-09-01|2019-12-01|TPLROC2AXO4FZRZ|2012-08-12|2012|    8| 12|Entertainment| 6.99|      seattle|\n",
      "|CPPEQURUHV|2012-08-01|2020-07-01|TD9GLV4KWNOMDA5|2020-05-24|2020|    5| 24|    Groceries| 8.23|  los_angeles|\n",
      "|CLJUTXITPP|2011-02-01|2020-05-01|T69D6GHB7SAP7CD|2015-05-01|2015|    5|  1|Entertainment| 3.36|san_francisco|\n",
      "|CQ08SYFJM8|2011-02-01|2019-05-01|T6CZYRUFGJULEIR|2012-10-26|2012|   10| 26|Entertainment| 22.7|       boston|\n",
      "|C0YDPQWPBJ|2010-03-01|2020-02-01|TZF0ZQV9PZSXASB|2011-04-23|2011|    4| 23|Entertainment| 7.69|    san_diego|\n",
      "|C74WTP3L6T|2011-08-01|2019-09-01|TUL4SX2XGT6XYTG|2012-10-10|2012|   10| 10|    Groceries|77.27|san_francisco|\n",
      "|C0YDPQWPBJ|2010-12-01|2019-05-01|TZVFZ4EQZZJVICM|2017-11-06|2017|   11|  6|Entertainment|17.83|       boston|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_file_csv = DATA_DIR + \"/TNX_test.csv\"\n",
    "df_transactions_csv = spark.read.parquet(transactions_file)\n",
    "df_transactions_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Convert from `*.csv` to `*.parqet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions_file_csv = DATA_DIR + \"/TNX.csv\"\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_transactions.repartition(5).write.mode(\"overwrite\").option(\"header\", \"true\").parquet(DATA_DIR + \"/TNX_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
